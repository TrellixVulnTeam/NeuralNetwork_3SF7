{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda36\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "input_dimension=784\n",
    "number_of_nodes_in_first_hidden_layer=300\n",
    "number_of_classes=10\n",
    "x = tf.placeholder(tf.float32, [None, input_dimension])\n",
    "one_hot_true_label = tf.placeholder(tf.float32, [None, number_of_classes])\n",
    "W1 = tf.Variable(tf.random_normal([input_dimension, number_of_nodes_in_first_hidden_layer], stddev=0.01), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([number_of_nodes_in_first_hidden_layer]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([number_of_nodes_in_first_hidden_layer,number_of_classes], stddev=0.01), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([number_of_classes]), name='b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_hidden_layer_net = tf.add(tf.matmul(x, W1), b1)\n",
    "first_hidden_layer_out = tf.nn.relu(first_hidden_layer_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_net=tf.add(tf.matmul(first_hidden_layer_out, W2), b2)\n",
    "# Calculate class probabilities by using softmax\n",
    "output_predicted_probabilities = tf.nn.softmax(output_layer_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "$$ \\large J =  - {1 \\over M}\\sum\\limits_{i = 1}^M {\\sum\\limits_{j = 1}^N {\\left[ {y_j^{(i)}\\log (\\hat y_j^{(i)}) + \\left( {1 - y_j^{(i)}} \\right)\\log (1 - \\hat y_j^{(i)})} \\right]} } $$\n",
    "\n",
    "where M is the number of training samples and N is the number of nodes in the output layer (number of classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ouput_clipped = tf.clip_by_value(output_predicted_probabilities, 1e-10, 0.9999999)\n",
    "cross_entropy_loss = -tf.reduce_mean(tf.reduce_sum(one_hot_true_label * tf.log(ouput_clipped)\n",
    "                         + (1 - one_hot_true_label) * tf.log(1 - ouput_clipped), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilizer Operator\n",
    "\n",
    "Note: This is only an object which needs to be executed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_operator = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Accuracy of the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(one_hot_true_label, 1), tf.argmax(output_predicted_probabilities, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1 Loss = 0.742\n",
      "Epoch # 2 Loss = 0.298\n",
      "Epoch # 3 Loss = 0.229\n",
      "Epoch # 4 Loss = 0.200\n",
      "Epoch # 5 Loss = 0.170\n",
      "Epoch # 6 Loss = 0.151\n",
      "Epoch # 7 Loss = 0.141\n",
      "Epoch # 8 Loss = 0.130\n",
      "Epoch # 9 Loss = 0.116\n",
      "Epoch # 10 Loss = 0.110\n",
      "Epoch # 11 Loss = 0.097\n",
      "Epoch # 12 Loss = 0.088\n",
      "Epoch # 13 Loss = 0.085\n",
      "Epoch # 14 Loss = 0.081\n",
      "Epoch # 15 Loss = 0.072\n",
      "Epoch # 16 Loss = 0.064\n",
      "Epoch # 17 Loss = 0.057\n",
      "Epoch # 18 Loss = 0.052\n",
      "Epoch # 19 Loss = 0.058\n",
      "Epoch # 20 Loss = 0.049\n",
      "Epoch # 21 Loss = 0.039\n",
      "Epoch # 22 Loss = 0.037\n",
      "Epoch # 23 Loss = 0.044\n",
      "Epoch # 24 Loss = 0.030\n",
      "Epoch # 25 Loss = 0.030\n",
      "Epoch # 26 Loss = 0.027\n",
      "Epoch # 27 Loss = 0.021\n",
      "Epoch # 28 Loss = 0.017\n",
      "Epoch # 29 Loss = 0.019\n",
      "Epoch # 30 Loss = 0.016\n",
      "Epoch # 31 Loss = 0.015\n",
      "Epoch # 32 Loss = 0.013\n",
      "Epoch # 33 Loss = 0.009\n",
      "Epoch # 34 Loss = 0.005\n",
      "Epoch # 35 Loss = 0.004\n",
      "Epoch # 36 Loss = 0.003\n",
      "Epoch # 37 Loss = 0.003\n",
      "Epoch # 38 Loss = 0.002\n",
      "Epoch # 39 Loss = 0.002\n",
      "Epoch # 40 Loss = 0.002\n",
      "Epoch # 41 Loss = 0.002\n",
      "Epoch # 42 Loss = 0.002\n",
      "Epoch # 43 Loss = 0.002\n",
      "Epoch # 44 Loss = 0.002\n",
      "Epoch # 45 Loss = 0.002\n",
      "Epoch # 46 Loss = 0.002\n",
      "Epoch # 47 Loss = 0.001\n",
      "Epoch # 48 Loss = 0.001\n",
      "Epoch # 49 Loss = 0.001\n",
      "Epoch # 50 Loss = 0.001\n",
      "Epoch # 51 Loss = 0.001\n",
      "Epoch # 52 Loss = 0.001\n",
      "Epoch # 53 Loss = 0.001\n",
      "Epoch # 54 Loss = 0.001\n",
      "Epoch # 55 Loss = 0.001\n",
      "Epoch # 56 Loss = 0.001\n",
      "Epoch # 57 Loss = 0.001\n",
      "Epoch # 58 Loss = 0.001\n",
      "Epoch # 59 Loss = 0.001\n",
      "Epoch # 60 Loss = 0.001\n",
      "Epoch # 61 Loss = 0.001\n",
      "Epoch # 62 Loss = 0.001\n",
      "Epoch # 63 Loss = 0.001\n",
      "Epoch # 64 Loss = 0.001\n",
      "Epoch # 65 Loss = 0.001\n",
      "Epoch # 66 Loss = 0.001\n",
      "Epoch # 67 Loss = 0.001\n",
      "Epoch # 68 Loss = 0.001\n",
      "Epoch # 69 Loss = 0.001\n",
      "Epoch # 70 Loss = 0.001\n",
      "Epoch # 71 Loss = 0.001\n",
      "Epoch # 72 Loss = 0.001\n",
      "Epoch # 73 Loss = 0.001\n",
      "Epoch # 74 Loss = 0.001\n",
      "Epoch # 75 Loss = 0.001\n",
      "Epoch # 76 Loss = 0.001\n",
      "Epoch # 77 Loss = 0.001\n",
      "Epoch # 78 Loss = 0.001\n",
      "Epoch # 79 Loss = 0.001\n",
      "Epoch # 80 Loss = 0.001\n",
      "Epoch # 81 Loss = 0.001\n",
      "Epoch # 82 Loss = 0.001\n",
      "Epoch # 83 Loss = 0.001\n",
      "Epoch # 84 Loss = 0.001\n",
      "Epoch # 85 Loss = 0.001\n",
      "Epoch # 86 Loss = 0.001\n",
      "Epoch # 87 Loss = 0.001\n",
      "Epoch # 88 Loss = 0.001\n",
      "Epoch # 89 Loss = 0.001\n",
      "Epoch # 90 Loss = 0.001\n",
      "Epoch # 91 Loss = 0.001\n",
      "Epoch # 92 Loss = 0.001\n",
      "Epoch # 93 Loss = 0.001\n",
      "Epoch # 94 Loss = 0.001\n",
      "Epoch # 95 Loss = 0.001\n",
      "Epoch # 96 Loss = 0.001\n",
      "Epoch # 97 Loss = 0.001\n",
      "Epoch # 98 Loss = 0.001\n",
      "Epoch # 99 Loss = 0.001\n",
      "Epoch # 100 Loss = 0.001\n",
      "Accuracy:  0.9755\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_operator)\n",
    "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            merge = tf.summary.merge_all()\n",
    "            _, loss = sess.run([optimizer, cross_entropy_loss], \n",
    "                         feed_dict={x: batch_x, one_hot_true_label: batch_y})\n",
    "            avg_loss += loss / total_batch\n",
    "            #print(\"Batch #\",i,  \"Avg. Loss : \", avg_loss)\n",
    "            \n",
    "        print(\"Epoch #\", (epoch + 1), \"Loss =\", \"{:.3f}\".format(avg_loss))\n",
    "    accuracy= sess.run(accuracy, feed_dict={x: mnist.test.images, one_hot_true_label: mnist.test.labels})\n",
    "    print(\"Accuracy: \",accuracy)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
